%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%\documentclass{article}
\documentclass[a4paper,11pt]{exam}


\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bbold}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{subfig}


\printanswers
\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\usepackage{titlesec}


%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment 3:Neural Networks \\ Object recognition and computer vision} % Title
\author{Thomas \textsc{Opsomer}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date


%----------------------------------------------------------------------------------------
%	Assignments 2
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	Part I
%----------------------------------------------------------------------------------------

\section{Training a neural network by back-propagation}

\subsection{Computing gradients of the loss with respect to network parameters}

\textbf{QA:In your report, derive using the chain rule the form of the gradient of the logistic loss (4) with respect to the parameters of the network Wi , Wo , Bi and Bo.\\}

To compute the gradient of s, we have to compute those 4 derivatives :
$\frac{\partial s}{\partial W_i}$,
$\frac{\partial s}{\partial B_i}$,
$\frac{\partial s}{\partial W_o}$,
$\frac{\partial s}{\partial B_o}$

Using the chain rule we have:
\[
\frac{\partial s}{\partial W_i}
   = \frac{\partial s}{\partial \bar{Y}} \frac{\partial \bar{Y}}{\partial H} \frac{\partial H}{\partial W_i}
\text{, }
\frac{\partial s}{\partial B_i}
   = \frac{\partial s}{\partial \bar{Y}} \frac{\partial \bar{Y}}{\partial H} \frac{\partial H}{\partial B_i}
\]
\[
\frac{\partial s}{\partial W_o}
   = \frac{\partial s}{\partial \bar{Y}} \frac{\partial \bar{Y}}{\partial W_o}
\text{, }
\frac{\partial s}{\partial B_o}
   = \frac{\partial s}{\partial \bar{Y}} \frac{\partial \bar{Y}}{\partial B_o}
\]

Then we compute each small partial derivative.

\[
\frac{\partial s}{\partial \bar{Y}}
   = \frac{Y}{1 + \exp(Y\bar{Y})}
\text{, }
\frac{\partial \bar{Y}}{\partial H}
   = W_o
\]
\[
\frac{\partial \bar{Y}}{\partial W_o}
   = H
\text{, } 
\frac{\partial \bar{Y}}{\partial B_o}
   = 1
\]
\[
\frac{\partial \bar{H}}{\partial W_i}
   = \mathbb 1_{\{W_iX + Bi > 0\}} * X'
\text{, } 
\frac{\partial \bar{Y}}{\partial B_i}
   = \mathbb 1_{\{W_iX + Bi > 0\}}
\]
That leads us to the different component of the loss gradient:

\[
\frac{\partial s}{\partial W_i} =
	\frac{Y}{1 + \exp(Y\bar{Y})} \cdot W_o \cdot \mathbb 1_{\{W_iX + B_i > 0\}} * X'
\]
\[
\frac{\partial s}{\partial B_i} =
	\frac{Y}{1 + \exp(Y\bar{Y})} \cdot W_o \cdot  \mathbb 1_{\{W_iX + B_i > 0\}}
\]
\[
\frac{\partial s}{\partial W_o} =
	\frac{Y}{1 + \exp(Y\bar{Y})}  \cdot ReLU(W_iX + B_i)
\]
\[
\frac{\partial s}{\partial B_o} =
	\frac{Y}{1 + \exp(Y\bar{Y})}
\]
with $\bar{Y} = W_o * ReLU(W_iX + B_i) + B_o$

\subsection{Numerically verify the gradients}

\textbf{QB1: In your report, write down the general formula for numerically computing the approximate derivative of the loss , with respect to the parameter  using finite differencing.\\}

Using the Taylor formula we have at the first order:

\[
f(\theta + \delta(\theta)) = f(\theta) + \delta(\theta) \cdot \nabla(f) + o(\delta(\theta))
\]
\[
\text{with } \nabla(f) \text{ the vector of coordinate } \frac{\partial f}{\partial \theta_{i}}
\]
Consequently we can compute a good numeric approximation of the derivative of $f$ in respect of the $i$ dimension by looking at the difference of $f$ in the direction $i$ for a $\delta$ small enough:
\[
\frac{\partial(f)}{\partial \theta_{i}} = \frac{f(\theta + \delta e_{i}) - f(\theta) }{\delta}
\]
\textbf{QB2: In your report, choose a training example {X,Y} and report the values of the analytically computed gradients :  grad\_s\_Wi, grad\_s\_Wo, grad\_s\_bi, grad\_s\_bo as well as their numerically computed counterparts: grad\_s\_Wi\_approx, grad\_s\_Wo\_approx, grad\_s\_bi\_approx, grad\_s\_bo\_approx. Are their values similar?\\}

Using the \verb|gradient_nn_approx| function we computed the approximation of gradient using the analytical formal above for a small $\delta$. As some gradient are of dimension $>$ 1, we computed the norm between the approximation and the one computed using backprop. We see that fortunately the values are similar.

\begin{center}
$|| grad\_s\_Wi\_approx - grad\_s\_Wi ||_{2} =  0.0968$\\
$|| grad\_s\_Wo\_approx - grad\_s\_Wo ||_{2} = 0.4738$\\
$|| grad\_s\_bi\_approx - grad\_s\_bi ||_{2} = 0.0546$\\
$|| grad\_s\_bo\_approx - grad\_s\_bo ||_{2} = 0.00032$
\end{center}


\subsection{Training the network using backpropagation and experimenting with different parameters}

\textbf{QC1: Include the decision hyper-plane visualization and the training and validation error plots in your report. What are the final training and validation errors? After how many iterations did the network converge?\\}

Figure \ref{hyperplane} shows the final decision hyper-plane after 30 iterations over the dataset. Looking at figure \ref{error} showing error evolution on both training and test set, we see that after 10 iteration has reached a good level of convergence. (dataset size is 1000, so one iteration consist of a pass on 1000 datapoint). The network achieved a full convergence to 0 after 32 iterations.\\ 

\begin{figure}[!h]
\centering
\includegraphics[width=13cm]{figures/hyperplane.eps}
\caption{Decision hyper-plane on the two moons dataset using one hidden layer with 7 hidden unit and  a learning rate of 0.2}    
\label{hyperplane}
\end{figure}

\begin{figure}[!h]%
    \centering
    \subfloat[]{{\includegraphics[width=6.5cm]{figures/train_err.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=6.5cm]{figures/val_err.eps} }}%
    \caption{Evolution of error on the training set on the left (a)\\
     and  on the validation set (b).}%
    \label{error}%
\end{figure}

\textbf{QC2: Random initializations. Repeat this procedure 5 times from 5 different random initializations. Record for each run the final training and validation errors. Did the network always converge to zero training error?\\}

Here we show training and validation error obtained on 5 different run of the classification algorithm. We see that most of the time the network converges, however to it can get stuck: for instance on the 5th run, it didn't converged and stayed at about 9\% of error in the validation set. 
\begin{center}
\begin{tabular}{ c | c | c }
	\# run & train error & test error \\
	\hline
	   1 & 0.0010 & 0\\
           2 & 0 & 0\\
         3 & 0 & 0\\
         4 & 0 & 0\\
    	 5 & 0.0730 & 0.0920\\
	 \hline
\end{tabular}
\end{center}

\textbf{QC3: Learning rate. Keep h=7 and change the learning rate to values $lrate = {2, 0.2, 0.02, 0.002}$. For each of these values run the training procedure 3 times and observe the training behaviour. For each run and the value of the learning rate report: the final (i) training and (ii) validation errors, and (iii) after how many iterations the network converged (if at all). Visualize the decision hyperplane and the evolution of the training error for each value of the learning (here only one of the three runs is sufficient). Briefly discuss the different behaviour of the training for different learning rates.\\}

The influence of the learning rate is showed on figure \ref{lrate_influence}, and numeric results in the following table. We observe that for very big or very small learning rate the network didn't reach convergence. In the big learning rate configuration, we see that the network oscillate between two values, this is due to gradient descent step being to big, we never reach the "hollow of the valley". On the opposite when the learning rate is very small, we don't reach convergence neither. Even if the error is still decreasing, steps are two small, we are never gonna find the minimum. The other values of 0.2 and 0.02, allow the network to converge, however we see that picking the best value (here 0.2) allows to converge much faster (7 iterations rather than 31 iterations), it is much more time and computationally efficient.

\begin{figure}[!h]
\centering
\includegraphics[width=15cm]{figures/lrate_influence.eps}
\caption{Influence of learning rate on network convergence. Error on test set in function of number of iterations}    
\label{lrate_influence}
\end{figure}


\begin{center}
\begin{tabular}{c |  c | c | c }
	\# lrate & final train error & final test error & converge step \\
	\hline
	   2 & 0.4760 & 0.4840 & - \\
           0.2 & 0 & 0 & 7\\
           0.02 & 0 & 0 & 31\\
           0.002 & 0.07 & 0.0810 & -\\
	 \hline
\end{tabular}
\end{center}



\textbf{QC4: The number of hidden units. Set the learning rate to 0.02 and change the number of hidden units $h = {1, 2, 5, 7, 10, 100}$. For each of these values run the training procedure 3 times and observe the training behaviour. For each run and the value of the number of hidden units record and report: the value of the final (i) training and (ii) validation error, and (iii) after how many iterations the network converged (if at all). Show the plot of the final decision hyperplane for each value of h (only one of the three plots for each h is sufficient). Discuss the different behaviours for the different numbers of hidden units.\\}

Using too few hidden unit doesn't give the freedom to the network to learn correctly the right frontier that split correctly the dataset. Here we see that we need at least 5 hidden unit to make the model converge.
Looking at the decision hyperplane for various number (cf figure \ref{h_hyp}) of hidden unit shows that starting at $h = 5$ the decision frontier correctly split the dataset but in a more rectilign fashion, whereas giving the network more hidden units make it more flexible and round. However we see that  using to much hidden unit doesn't really improve the decision, but make it just smoothier and it may not be worth it to pay as much computation for that.
Concerning the influence of the number of unit on the convergence speed, we see that hidden layer with many hidden units seems to converge faster in term of iterations during the first iterations. However using more units increase the amount computation and consequently slow the learning because of more parameters to learn.

\begin{center}
\begin{tabular}{ c | c |  c | c | c }
	\#run & h & final train error & final test error & converge step \\
	\hline
	   1 & 1 & 0.0890 & 0. 1140 & - \\  
	   2 & 1 & 0.0930 &  0.1150 & - \\
	   3 & 1 & 0.0980 & 0.1180 & - \\
	   \hline
	   1 & 2 & 0.0870 & 0.1120 & - \\
	   2 & 2 & 0.0930 & 0.1120 & - \\
	   3 & 2 & 0.0860 & 0.1070 & - \\
	   \hline
	   1 & 5 & 0.0010 & 0 & 23 \\
	   2 & 5 & 0 & 0 & 25 \\
	   3 & 5 & 0.0830 & 0.0780 & - \\
	   \hline
	   1 & 7 & 0 & 0 & 17 \\
	   2 & 7 & 0 & 0 &  13 \\
	   3 & 7 & 0.0610 & 0.0770 & - \\
	   \hline
	   1 & 10 & 0 & 0 & 19 \\
	   2 & 10 & 0 & 0 & 19 \\
	   3 & 10 & 0 & 0 & 12 \\
	   \hline
	   1 & 100 & 0 & 0 & 18 \\
	   2 & 100 & 0 & 0 & 11 \\
	   3 & 100 & 0 & 0 & 12 \\
	 \hline
\end{tabular}
\end{center}


\begin{figure}[!h]
\centering
\includegraphics[width=15cm]{figures/h_influence.eps}
\caption{Influence of number of hidden unit on the network convergence. Error on test set in function of number of iterations}    
\label{h_influence}
\end{figure}


\begin{figure}[!h]%
    \centering
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h1_hyp.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h2_hyp.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h5_hyp.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h7_hyp.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h10_hyp.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=4.5cm]{figures/h100_hyp.eps} }}%
    \qquad
    \caption{Decision hyperplane for different number of hidden unit: a: 1, b: 2 c: 5, d: 7, e: 10, f:100}%
    \label{h_hyp}%
\end{figure}

\clearpage

\section{Image classification with CNN features}

\subsection{Computing CNN features}

\textbf{Q2A1: What is done by the above normalization code and why is it needed?\\}

This normalization code does two things:
\begin{enumerate}
\item First it resize the image in order to have homogeneous size among image and also I guess to save some time in computation as bigger images cause more computation.
\item Then we "center" our image in a statistical term: we subtract the mean of all images to our images. During the network training, inputs were centered to allow the optimization to be more effective. Indeed it make the update step (in SGD) more significative and prevent situation where the update is small in average  but each update are huge (positively and negatively). Consequently we need to center our image to be in a situation similar to training.
\end{enumerate}


\textbf{Q2A2: Look at the content of variables net and res. What is their structure? What is encoded by net.layers for different values of k? What do values in res(k).x represent for different values of k?\\}

$net$ is a struct with 2 fields: $meta$ and $layers$ and $res$ is an array of struct with several fields also. In net.layers are encoded each layer, each step of the whole convolutional network. For instance in $net.layers\{11\}$ is encoded the fourth convolutional layer, with its size, its, stride, its weights... The structure of $res$ and $net$ are obviously linked: The value $res(k).x$ correspond to the output of the $k-1$ layer of the $net$.

\subsection{Image classification using CNN features and linear SVM}

\textbf{Q2B1: Report your classification results for the three object classes by including precision-recall plots and AP values into the report. What CNN layers work best as features? Are feature normalization and SVM cross-validation important steps to obtain best results? Motivate your answer by performance numbers. ImageNet dataset contains no person class. Can you connect this fact to the difference in the performance of your person classifiers trained/tested using different layers of CNN output?\\}

We found by cross-validation that a good parameter for C for every classifier is 5. The model wasn't very much sensitive to variation of the C parameter. For very small value it did react quite worst however for value greater than 1, difference wasn't huge. \\

Figure \ref{cnn} shows results (Error on the validation set and top 36 ranked results) on every category for a Linear SVM, using the output of the CNN "fc8" layer as features and using L2-normalization.\\

We have done some experimentation in order to see which features seems more effective and also how normalization is impacting performance. The following two tables show for each class and for each category the Average Precision of the classifier without normalization on the first one and with L2 normalization of the second.

\begin{center}
\begin{tabular}{ c | c | c |  c }
	category & soft-max & fc7 & fc8 \\
	\hline
	   motorbike &  0.9073 & 0.9007 & 0.9112 \\
	   person  & 0.9049 & 0.9464 & 0.9376 \\
	   aeroplane &  0.9422 & 0.9206 & 0.9080 \\
	 \hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{ c | c | c |  c }
	category & soft-max + L2 & fc7 + L2 & fc8 + L2 \\
	\hline
	   motorbike &  0.8805 & 0.9257 & 0.9295 \\
	   person  & 0.8941  &  0.9541  & 0.9559 \\
	   aeroplane &  0.9257  &  0.9299  &  0.9395 \\
	 \hline
\end{tabular}
\end{center}

We observe that without normalization, it is hard to choose a single feature as being the best one. Indeed soft-max perform better for aeroplanes but fc7 for person. However using L2-normalization give more credit to the feature coming from the fc8 layer as for each class performance is better using fc8 layer plus normalization. However we see that performance on aeroplane was better using the soft-max layer and no normalzation. Anyway on a general view using L2-normalization seems to make the model more robust and efficient.\\

We also tried using a concatenation of several of these features, but it didn't bring any improvement on test set. Actually the model started to overfit even more, as the AP was 100\% on training data but AP decrease on test data.\\

Our SVM classifier performs well on the class person with a performance of $95.5\%$ using f8 features. Which can be strange given the fact the the ImageNet contains no person class. Actually it could explain the fact that using the soft-max layer couldn't make performance higher than 0.9. Because if the ImageNet doesn't contains person class it mean that the final soft-max layer doesn't know which class to select has to real class doesn't exist and brings more uncertainty and error. On the other hand even if ImageNet contains no person class it does contains many images with people and many classes were people are distinctive part of the images consequently it is normal the network achieved to learn effective representation of people.\\


\begin{figure}[!h]%
    \centering
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/aeroplane_fc8_l2_ap_val.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/aeroplane_fc8_l2_36.jpg} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/person_fc8_l2_ap_val.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/person_fc8_l2_36.jpg} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/motorbike_fc8_l2_ap_val.eps} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=7cm]{figures/Q23/motorbike_fc8_l2_36.jpg} }}%
    \qquad
    \caption{For each class, result of the Linear SVM, trained on the features extracted from the layer $fc8$ of the Convolution Neural Network. Features are also L2-normalized}%
    \label{cnn}%
\end{figure}


\textbf{Q2B2: Compare your best results using CNN features to your results obtained in Assignment 2 for corresponding object classes and training/testing image subsets. What conclusions can you draw? Illustrate a few negative test samples with the highest classification scores (false positives) and a few positive test samples with the lowest classification scores (false negatives) for each class. Can you explain the errors of the classifier?\\}

In the assignment 2, we achieved the best results with Fisher Vector using a Hellinger Kernel. The following table compares best results from assignment 2 and assignment 3. We see that improvements are huge. Arguably we can say that features learned in the convolution neural network at each layer are way more efficient, representative and consistent as they overcome some finest hand-builded features like Fisher Vectors in a simple Linear Classifier.

\begin{center}
\begin{tabular}{ c | c | c}
	category & Assigment 2 & Assignment 3 \\
	\hline
	   motorbike &  0.7825 & 0.9295 \\
	   person  & 0.7914  &  0.9559 \\
	   aeroplane &  0.7720  &  0.9395 \\
	 \hline
\end{tabular}
\end{center}

Figures \ref{motorbad}, \ref{personbad}, \ref{aeroplanebad} show the first three false positive and false negative for respectively each classifier.

\begin{enumerate}
\item Motorbike classifier. We observe that false positive are kind of strange. There is a picture of a boat on a beach, picture inside a building with column and flower, and a bike near a tante. If the bike could be mismatch with a motorbike, result for the first two false positive are very strange and hard to explain. Concerning the false negative, we see that motorbike in them are quite hard to find, they are dispersed within a rich context, or we can see only part of them.
\item Person classifier. We observe again a strange thing with the picture of the boat at the beach appearing again. But actually those three pictures shows context were it is likely to see a person (inside a car, a chair). Concerning the false negative, here again, picture are very hard to classify even for a human. For instance on the first one, one need to focus to see the litlle head of the person on the coach whose body is hidden.
\item Aeroplane classifier. Here false positive are more explainable: it mismatches a bird in the sky and a pirogue with a plane, but their form actually are similar to an airplane. False negative are more troubling. There is an old 'sepia' picture that can be hard to deal with for the CNN features, but the other two are simple plane at airport and in the sky.
\end{enumerate}



\begin{figure}[!h]%
    \centering
    \subfloat[The 3 better ranked false positives]{{\includegraphics[width=15cm]{figures/motorbike_first_false_positive.jpg} }}%
    \qquad
    \subfloat[The 3 worst ranked false positive]{{\includegraphics[width=15cm]{figures/motorbike_first_false_negative.jpg} }}%
    \caption{False positive and false negative for the motorbike classifier.}%
    \label{motorbad}%
\end{figure}


\begin{figure}[!h]%
    \centering
    \subfloat[The 3 better ranked false positives]{{\includegraphics[width=15cm]{figures/person_first_false_positive.jpg} }}%
    \qquad
    \subfloat[The 3 worst ranked false positive]{{\includegraphics[width=15cm]{figures/person_first_false_negative.jpg} }}%
    \caption{False positive and false negative for the person classifier.}%
    \label{personbad}%
\end{figure}

\begin{figure}[!h]%
    \centering
    \subfloat[The 3 better ranked false positives]{{\includegraphics[width=15cm]{figures/aeroplane_first_false_positive.jpg} }}%
    \qquad
    \subfloat[The 3 worst ranked false positive]{{\includegraphics[width=15cm]{figures/aeroplane_first_false_negative.jpg} }}%
    \caption{False positive and false negative for the aeroplane classifier.}%
    \label{aeroplanebad}%
\end{figure}

Finally we see that errors of classifiers are for some easy to explain especially for false negative where we see that the classifier need more training images with more diversity. However some false positive are hard to explain and I guess point out strange features extraction from the CNN.


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{apalike}

\bibliography{sample}

%----------------------------------------------------------------------------------------


\end{document}